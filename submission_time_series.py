# -*- coding: utf-8 -*-
"""Submission_Time_Series.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CcTrgjBiM84QCibCWuXNHjToGOsjn2Ux

# **Apple Stock Prices**
"""

!pip install -q kaggle

from google.colab import files
#Upload Kaggle API Key
files.upload()

!mkdir ~/.kaggle #make new directory in root folder
!cp kaggle.json ~/.kaggle/ #copy and paste kaggle API key to new directory
!chmod 600 ~/.kaggle/kaggle.json #permission
!kaggle datasets list

!kaggle datasets download -d meetnagadia/apple-stock-price-from-19802021

import numpy as np
import zipfile
import pandas as pd
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.model_selection import train_test_split

zipPath = '../content/apple-stock-price-from-19802021.zip' #zip path in local
zipFile = zipfile.ZipFile(zipPath, 'r')
zipFile.extractall('../content/AppleStock') #extract to new directory
zipFile.close() #close connection to object

data = pd.read_csv('/content/AppleStock/AAPL.csv')
data

data.isnull().sum()

data = data[['Date', 'Close']]

data.head()

data.info()

mae_rate = (data['Close'].max() - data['Close'].min()) * 10/100
print(mae_rate)

features = data.drop('Close', axis=1)  # Adjust this based on your feature columns
target = data['Close']

Date = data['Date'].values
Price = data['Close'].values

plt.figure(figsize=(20,8))
plt.plot(Date, Price)
plt.title('Apple Stock Price')
plt.xlabel('Date')
plt.ylabel('Price')
plt.show()

x_train, x_test, y_train, y_test = train_test_split(Price, Date, test_size = 0.2)

print('Jumlah Data Train : ',len(x_train))
print('Jumah Data Test : ',len(x_test))

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
  series = tf.expand_dims(series, axis=-1)
  data_series = tf.data.Dataset.from_tensor_slices(series)
  data_series = data_series.window(window_size + 1, shift=1, drop_remainder = True)
  data_series = data_series.flat_map(lambda w: w.batch(window_size + 1))
  data_series = data_series.shuffle(shuffle_buffer)
  data_series = data_series.map(lambda w: (w[:-1], w[-1:]))
  return data_series.batch(batch_size).prefetch(1)

data_training = windowed_dataset(x_train, window_size=64, batch_size=128, shuffle_buffer=1000)
data_testing = windowed_dataset(x_test, window_size=64, batch_size=128, shuffle_buffer=1000)

model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(64, return_sequences=True),
  tf.keras.layers.LSTM(64),
  tf.keras.layers.Dense(30, activation="relu"),
  tf.keras.layers.Dropout(0.5),
  tf.keras.layers.Dense(10, activation="relu"),
  tf.keras.layers.Dropout(0.5),
  tf.keras.layers.Dense(1),
])

optimizers = tf.keras.optimizers.SGD(learning_rate=1.0000e-04, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizers,
              metrics=["mae"])
model_history = model.fit(
    data_training,
    epochs=50,
    validation_data=data_testing,
)